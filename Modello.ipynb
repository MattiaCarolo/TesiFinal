{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import hashlib\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "from progress.bar import Bar\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from Utils.NotesSeq import NoteSeq as ns\n",
    "from Utils.EventSeq import EventSeq as es\n",
    "from Utils.ControlSeq import ControlSeq as cs\n",
    "from Utils import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Gumbel\n",
    "from torch import optim\n",
    "\n",
    "from config import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretty_midi import PrettyMIDI, Note, Instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_path = 'save/train.sess'\n",
    "data_path = 'Processed-RAW'\n",
    "saving_interval = 60.\n",
    "reset_optimizer = False\n",
    "enable_logging = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, root, verbose=False):\n",
    "        assert os.path.isdir(root), root\n",
    "        paths = utils.find_files_by_extensions(root, ['.data'])\n",
    "\n",
    "        self.root = root\n",
    "        self.samples = []\n",
    "        self.seqlens = []\n",
    "        self.samples2 = []\n",
    "        self.seqlens2 = []\n",
    "\n",
    "        if verbose:\n",
    "            paths = Bar(root).iter(list(paths))\n",
    "        for path in paths:\n",
    "            eventseq, eventseq2, controlseq, controlseq2 = torch.load(path)\n",
    "            controlseq = cs.recover_compressed_array(controlseq)\n",
    "            controlseq2 = cs.recover_compressed_array(controlseq2)\n",
    "            assert len(eventseq) == len(controlseq)\n",
    "            assert len(eventseq2) == len(controlseq2)\n",
    "            self.samples.append((eventseq, controlseq))\n",
    "            self.seqlens.append(len(eventseq))\n",
    "            self.samples2.append((eventseq2,controlseq2))\n",
    "            self.seqlens2.append(len(eventseq2))\n",
    "\n",
    "        self.avglen = np.mean(self.seqlens)\n",
    "        self.avglen2 = np.mean(self.seqlens2)\n",
    "    \n",
    "    def batches(self, batch_size, window_size, stride_size):\n",
    "        indeces = [(i, range(j, j + window_size))\n",
    "                   for i, seqlen in enumerate(self.seqlens)\n",
    "                   for j in range(0, seqlen - window_size, stride_size)]\n",
    "        while True:\n",
    "            eventseq_batch = []\n",
    "            controlseq_batch = []\n",
    "            eventseq_batch2 = []\n",
    "            controlseq_batch2 = []\n",
    "            n = 0\n",
    "            for ii in np.random.permutation(len(indeces)):\n",
    "                i, r = indeces[ii]\n",
    "\n",
    "                eventseq, controlseq = self.samples[i]\n",
    "                eventseq2, controlseq2 = self.samples2[i]\n",
    "\n",
    "                eventseq = eventseq[r.start:r.stop]\n",
    "                eventseq2 = eventseq2[r.start:r.stop]\n",
    "\n",
    "                controlseq = controlseq[r.start:r.stop]\n",
    "                controlseq2 = controlseq2[r.start:r.stop]\n",
    "\n",
    "                eventseq_batch.append(eventseq)\n",
    "                controlseq_batch.append(controlseq)\n",
    "                eventseq_batch2.append(eventseq2)\n",
    "                controlseq_batch2.append(controlseq2)\n",
    "\n",
    "                n += 1\n",
    "                if n == batch_size:\n",
    "                    yield (np.stack(eventseq_batch, axis=1),\n",
    "                           np.stack(controlseq_batch, axis=1),\n",
    "                           np.stack(eventseq_batch, axis=1),\n",
    "                           np.stack(controlseq_batch, axis=1))\n",
    "                    eventseq_batch.clear()\n",
    "                    controlseq_batch.clear()\n",
    "                    eventseq_batch2.clear()\n",
    "                    controlseq_batch2.clear()\n",
    "                    n = 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'Dataset(root=\"{self.root}\", '\n",
    "                f'samples={len(self.samples)}, '\n",
    "                f'avglen={self.avglen})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset.samples)\n",
    "assert dataset_size > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P2X(nn.Module):\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.event_embedding.weight)\n",
    "        nn.init.xavier_normal_(self.inithid_fc.weight)\n",
    "        self.inithid_fc.bias.data.fill_(0.)\n",
    "        nn.init.xavier_normal_(self.concat_input_fc.weight)\n",
    "        nn.init.xavier_normal_(self.output_fc.weight)\n",
    "        self.output_fc.bias.data.fill_(0.)\n",
    "\n",
    "    def __init__(self, event_dim, control_dim, init_dim, hidden_dim,\n",
    "                 inithid_fc = None, gru_layers=3, gru_dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters initialization\n",
    "        self.event_dim = event_dim\n",
    "        self.control_dim = control_dim\n",
    "        self.init_dim = init_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_layers = gru_layers\n",
    "        self.concat_dim = event_dim + 1 + control_dim\n",
    "        self.input_dim = hidden_dim\n",
    "        self.output_dim = event_dim\n",
    "\n",
    "        self.primary_event = self.event_dim - 1\n",
    "\n",
    "        #Model Layers\n",
    "\n",
    "        self.inithid_fc = nn.Linear(init_dim, gru_layers * hidden_dim)\n",
    "        self.inithid_fc_activation = nn.Tanh()\n",
    "\n",
    "        self.event_embedding = nn.Embedding(event_dim, event_dim)\n",
    "        self.concat_input_fc = nn.Linear(self.concat_dim, self.input_dim)\n",
    "        self.concat_input_fc_activation = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "        self.gru = nn.GRU(self.input_dim, self.hidden_dim,\n",
    "                          num_layers=gru_layers, dropout=gru_dropout)\n",
    "        self.output_fc = nn.Linear(hidden_dim * gru_layers, self.output_dim)\n",
    "        self.output_fc_activation = nn.Softmax(dim=-1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, event, control=None, hidden=None):\n",
    "        # One step forward\n",
    "\n",
    "        assert len(event.shape) == 2\n",
    "        assert event.shape[0] == 1\n",
    "        batch_size = event.shape[1]\n",
    "        event = self.event_embedding(event)\n",
    "\n",
    "        if control is None:\n",
    "            default = torch.ones(1, batch_size, 1).to(device)\n",
    "            control = torch.zeros(1, batch_size, self.control_dim).to(device)\n",
    "        else:\n",
    "            default = torch.zeros(1, batch_size, 1).to(device)\n",
    "            assert control.shape == (1, batch_size, self.control_dim)\n",
    "\n",
    "        concat = torch.cat([event, default, control], -1)\n",
    "        input = self.concat_input_fc(concat)  #nn.Linear(self.concat_dim, self.input_dim)\n",
    "        input = self.concat_input_fc_activation(input)  #nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "        _, hidden = self.gru(input, hidden)  #nn.GRU(self.input_dim, self.hidden_dim,num_layers=gru_layers, dropout=gru_dropout)\n",
    "\n",
    "        output = hidden.permute(1, 0, 2).contiguous()\n",
    "        output = output.view(batch_size, -1).unsqueeze(0)\n",
    "        output = self.output_fc(output) #nn.Linear(hidden_dim * gru_layers, self.output_dim)\n",
    "        return output, hidden # output is under the form of a logit\n",
    "    \n",
    "    def _sample_event(self, output, greedy=True, temperature=1.0):\n",
    "        if greedy:\n",
    "            return output.argmax(-1)\n",
    "        else:\n",
    "            output = output / temperature\n",
    "            probs = self.output_fc_activation(output)\n",
    "            return Categorical(probs).sample()\n",
    "    \n",
    "    def get_primary_event(self, batch_size):\n",
    "        return torch.LongTensor([[self.primary_event] * batch_size]).to(device)\n",
    "\n",
    "    def init_to_hidden(self, init):\n",
    "        # [batch_size, init_dim]\n",
    "        batch_size = init.shape[0]\n",
    "        out = self.inithid_fc(init)\n",
    "        out = self.inithid_fc_activation(out)\n",
    "        out = out.view(self.gru_layers, batch_size, self.hidden_dim)\n",
    "        return out\n",
    "    \n",
    "    def expand_controls(self, controls, steps):\n",
    "        # [1 or steps, batch_size, control_dim]\n",
    "        assert len(controls.shape) == 3\n",
    "        assert controls.shape[2] == self.control_dim\n",
    "        if controls.shape[0] > 1:\n",
    "            assert controls.shape[0] >= steps\n",
    "            return controls[:steps]\n",
    "        return controls.repeat(steps, 1, 1)\n",
    "    \n",
    "    def generate(self, init, batch_size, init_dim, steps, events = None, controls = None,\n",
    "                 verbose = True, greedy = 1.0, temperature = 1.0):\n",
    "        batch_size = batch_size\n",
    "        self.init_dim = init_dim\n",
    "\n",
    "        assert init.shape[1] == self.init_dim\n",
    "        assert steps > 0\n",
    "\n",
    "        use_teacher_forcing = events is not None\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            assert len(events.shape) == 2\n",
    "            assert events.shape[0] >= steps - 1\n",
    "            events = events[:steps-1]\n",
    "\n",
    "        event = self.get_primary_event(batch_size)\n",
    "\n",
    "        use_control = controls is not None\n",
    "\n",
    "        if use_control:\n",
    "            controls = self.expand_controls(controls, steps)\n",
    "        hidden = self.init_to_hidden(init)\n",
    "\n",
    "        outputs = []\n",
    "        step_iter = range(steps)\n",
    "\n",
    "        if verbose:\n",
    "            step_iter = Bar('Generating').iter(step_iter)\n",
    "\n",
    "        for step in step_iter:\n",
    "            #control = controls[step].unsqueeze(0) if use_control else None\n",
    "            control = None\n",
    "            output, hidden = self.forward(event, control, hidden)\n",
    "\n",
    "            use_greedy = np.random.random() < greedy\n",
    "            event = self._sample_event(output, greedy=use_greedy,\n",
    "                                       temperature=temperature)\n",
    "            \n",
    "            #here outputs are served in the lo-git format\n",
    "            outputs.append(output)\n",
    "            #\n",
    "            #if use_teacher_forcing and step < steps - 1: # avoid last one\n",
    "            #    if np.random.random() <= teacher_forcing_ratio:\n",
    "            #        event = events[step].unsqueeze(0)\n",
    "        \n",
    "        return torch.cat(outputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P2XSecondary(nn.Module):\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.event_embedding.weight)\n",
    "        nn.init.xavier_normal_(self.inithid_fc.weight)\n",
    "        self.inithid_fc.bias.data.fill_(0.)\n",
    "        nn.init.xavier_normal_(self.concat_input_fc.weight)\n",
    "        nn.init.xavier_normal_(self.output_fc.weight)\n",
    "        self.output_fc.bias.data.fill_(0.)\n",
    "\n",
    "    def __init__(self, event_dim, control_dim, init_dim, hidden_dim,\n",
    "                 inithid_fc = None, gru_layers=3, gru_dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters initialization\n",
    "        self.event_dim = event_dim\n",
    "        self.control_dim = control_dim\n",
    "        self.init_dim = init_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_layers = gru_layers\n",
    "        self.concat_dim = event_dim + event_dim + 1 + control_dim\n",
    "        self.input_dim = hidden_dim\n",
    "        self.output_dim = event_dim\n",
    "\n",
    "        self.primary_event = self.event_dim - 1\n",
    "\n",
    "        #Model Layers\n",
    "\n",
    "        self.inithid_fc = nn.Linear(init_dim, gru_layers * hidden_dim)\n",
    "        self.inithid_fc_activation = nn.Tanh()\n",
    "\n",
    "        self.event_embedding = nn.Embedding(event_dim, event_dim)\n",
    "        self.concat_input_fc = nn.Linear(self.concat_dim, self.input_dim)\n",
    "        self.concat_input_fc_activation = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "        self.gru = nn.GRU(self.input_dim, self.hidden_dim,\n",
    "                          num_layers=gru_layers, dropout=gru_dropout)\n",
    "        self.output_fc = nn.Linear(hidden_dim * gru_layers, self.output_dim)\n",
    "        self.output_fc_activation = nn.Softmax(dim=-1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, event, event2, control=None, hidden=None):\n",
    "        # One step forward\n",
    "\n",
    "        assert len(event.shape) == 2\n",
    "        assert event.shape[0] == 1\n",
    "        batch_size = event.shape[1]\n",
    "        event = self.event_embedding(event)\n",
    "\n",
    "        #print(event2.shape)\n",
    "        if(len(event2.shape) <= 2):\n",
    "            event2 = torch.tensor(event2).to(device).long()\n",
    "            event2 = self.event_embedding(event2)\n",
    "\n",
    "        \"\"\"\n",
    "        print(event2.shape)\n",
    "        print(event.shape)\n",
    "        print(len(event.shape))\n",
    "        \"\"\"\n",
    "\n",
    "        if control is None:\n",
    "            default = torch.ones(1, batch_size, 1).to(device)\n",
    "            control = torch.zeros(1, batch_size, self.control_dim).to(device)\n",
    "        else:\n",
    "            default = torch.zeros(1, batch_size, 1).to(device)\n",
    "            assert control.shape == (1, batch_size, self.control_dim)\n",
    "\n",
    "        event = torch.cat([event,event2],-1)\n",
    "        concat = torch.cat([event, default, control], -1)\n",
    "        input = self.concat_input_fc(concat)  #nn.Linear(self.concat_dim, self.input_dim)\n",
    "        input = self.concat_input_fc_activation(input)  #nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "        _, hidden = self.gru(input, hidden)  #nn.GRU(self.input_dim, self.hidden_dim,num_layers=gru_layers, dropout=gru_dropout)\n",
    "\n",
    "        output = hidden.permute(1, 0, 2).contiguous()\n",
    "        output = output.view(batch_size, -1).unsqueeze(0)\n",
    "        output = self.output_fc(output) #nn.Linear(hidden_dim * gru_layers, self.output_dim)\n",
    "\n",
    "        #print(\"prima passata\")\n",
    "\n",
    "        return output, hidden # output is under the form of a logit\n",
    "    \n",
    "    def _sample_event(self, output, greedy=True, temperature=1.0):\n",
    "        if greedy:\n",
    "            return output.argmax(-1)\n",
    "        else:\n",
    "            output = output / temperature\n",
    "            probs = self.output_fc_activation(output)\n",
    "            return Categorical(probs).sample()\n",
    "    \n",
    "    def get_primary_event(self, batch_size):\n",
    "        return torch.LongTensor([[self.primary_event] * batch_size]).to(device)\n",
    "\n",
    "    def init_to_hidden(self, init):\n",
    "        # [batch_size, init_dim]\n",
    "        batch_size = init.shape[0]\n",
    "        out = self.inithid_fc(init)\n",
    "        out = self.inithid_fc_activation(out)\n",
    "        out = out.view(self.gru_layers, batch_size, self.hidden_dim)\n",
    "        return out\n",
    "    \n",
    "    def expand_controls(self, controls, steps):\n",
    "        # [1 or steps, batch_size, control_dim]\n",
    "        assert len(controls.shape) == 3\n",
    "        assert controls.shape[2] == self.control_dim\n",
    "        if controls.shape[0] > 1:\n",
    "            assert controls.shape[0] >= steps\n",
    "            return controls[:steps]\n",
    "        return controls.repeat(steps, 1, 1)\n",
    "    \n",
    "    def generate(self, init, batch_size, init_dim, steps, events = None, controls = None, events2 = None,\n",
    "                 verbose = True, greedy = 1.0, temperature = 1.0, teacher_forcing_ratio = 1.0):\n",
    "        batch_size = batch_size\n",
    "        self.init_dim = init_dim\n",
    "\n",
    "        assert init.shape[1] == self.init_dim\n",
    "        assert steps > 0\n",
    "\n",
    "        use_teacher_forcing = events is not None\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            assert len(events.shape) == 2\n",
    "            assert events.shape[0] >= steps - 1\n",
    "            events = events[:steps-1]\n",
    "\n",
    "        event = self.get_primary_event(batch_size)\n",
    "        event2 = self.get_primary_event(batch_size)\n",
    "\n",
    "        use_control = controls is not None\n",
    "\n",
    "        if use_control:\n",
    "            controls = self.expand_controls(controls, steps)\n",
    "        hidden = self.init_to_hidden(init)\n",
    "\n",
    "        outputs = []\n",
    "        step_iter = range(steps)\n",
    "\n",
    "        if verbose:\n",
    "            step_iter = Bar('Generating').iter(step_iter)\n",
    "\n",
    "        for step in step_iter:\n",
    "            #control = controls[step].unsqueeze(0) if use_control else None\n",
    "            control = None\n",
    "            output, _ = self.forward(event, event2, control, hidden)\n",
    "\n",
    "            use_greedy = np.random.random() < greedy\n",
    "            event = self._sample_event(output, greedy=use_greedy,\n",
    "                                       temperature=temperature)\n",
    "            \n",
    "            #here outputs are served in the logit format\n",
    "            outputs.append(output)\n",
    "            #\n",
    "            if use_teacher_forcing and step < steps - 1: # avoid last one\n",
    "                if np.random.random() <= teacher_forcing_ratio:\n",
    "                    event = events[step].unsqueeze(0)\n",
    "                    event2 = events2[step].unsqueeze(0)\n",
    "        \n",
    "        outputs = torch.cat(outputs, 0)\n",
    "\n",
    "        return outputs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for training process\n",
    "\n",
    "init_dim = 32\n",
    "event_dim = es.dim()\n",
    "control_dim = cs.dim()\n",
    "hidden_dim = 512\n",
    "gru_layers = 3\n",
    "gru_droput = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "window_size = 200\n",
    "stride_size = 10\n",
    "use_transposition = False\n",
    "control_ratio = 1.0\n",
    "teacher_forcing_ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'init_dim': init_dim,\n",
    "    'event_dim': event_dim,\n",
    "    'control_dim': control_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'gru_layers': gru_layers,\n",
    "    'gru_dropout': gru_droput,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = torch.randn(batch_size, init_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = P2X(**model_config).to(device)\n",
    "model2 = P2XSecondary(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters()) + list(model.parameters())\n",
    "paramsB = list(model.parameters()) + list(model2.parameters())\n",
    "\n",
    "optimizer = optim.Adam(paramsB, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 10.96728515625\n",
      "iter 1, loss: 10.79328727722168\n",
      "iter 2, loss: 10.409486770629883\n",
      "iter 3, loss: 11.087179183959961\n",
      "iter 4, loss: 9.985830307006836\n",
      "iter 5, loss: 9.799489974975586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#try:\n",
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)\n",
    "for iteration, data in enumerate(batch_gen):\n",
    "    e1 = data[0]\n",
    "    c1 = data[1]\n",
    "    e2 = data[2]\n",
    "    c2 = data[3]\n",
    "\n",
    "    # First Model process\n",
    "\n",
    "    events = torch.LongTensor(e1).to(device)\n",
    "    assert events.shape[0] == window_size\n",
    "    assert len(events.shape) == 2\n",
    "    assert events.shape[0] >= window_size - 1\n",
    "\n",
    "    if np.random.random() < control_ratio:\n",
    "        controls = torch.FloatTensor(c1).to(device)\n",
    "        assert controls.shape[0] == window_size\n",
    "    else:\n",
    "        controls = None\n",
    "\n",
    "    init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "    outputs = model.generate(init,batch_size, model.init_dim, window_size, \n",
    "                            events[:-1], controls)\n",
    "    \n",
    "    assert outputs.shape[:2] == events.shape[:2]\n",
    "    loss = loss_function(outputs.view(-1, event_dim), events.view(-1))\n",
    "    #print(outputs)\n",
    "\n",
    "    # Second Model process\n",
    "\n",
    "    eventsB = torch.LongTensor(e2).to(device)\n",
    "    assert eventsB.shape[0] == window_size\n",
    "    assert len(eventsB.shape) == 2\n",
    "    assert eventsB.shape[0] >= window_size - 1\n",
    "\n",
    "    if np.random.random() < control_ratio:\n",
    "        controls = torch.FloatTensor(c2).to(device)\n",
    "        assert controls.shape[0] == window_size\n",
    "    else:\n",
    "        controls = None\n",
    "\n",
    "    init = torch.randn(batch_size, model2.init_dim).to(device)\n",
    "    outputsB = model2.generate(init,batch_size, model2.init_dim, window_size, \n",
    "                            events=eventsB[:-1], events2=outputs[:-1], controls=controls)\n",
    "    \n",
    "    assert outputsB.shape[:2] == eventsB.shape[:2]\n",
    "    #eventsF = torch.cat((events,eventsB))\n",
    "    #outputsF = torch.cat((outputs, outputsB),2)\n",
    "    #print(outputsF)\n",
    "\n",
    "    loss = loss + loss_function(outputsB.view(-1, event_dim), eventsB.view(-1))\n",
    "\n",
    "    model.zero_grad()\n",
    "    model2.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    #concatenated_vectors = torch.cat((outputs, outputsB),2)\n",
    "    #print(outputsF.shape)\n",
    "    #print(outputsB.shape)\n",
    "    #print(events.shape)\n",
    "\n",
    "    norm = utils.compute_gradient_norm(model.parameters())\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'iter {iteration}, loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "\n",
    "#except:\n",
    "#    print(\"banane\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagigi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
