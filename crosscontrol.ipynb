{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import hashlib\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "from progress.bar import Bar\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from Utils.NotesSeq import NoteSeq as ns\n",
    "from Utils.EventSeq import EventSeq as es\n",
    "from Utils.ControlSeq import ControlSeq as cs\n",
    "from Utils import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_midi(path):\n",
    "\n",
    "    note_seq = ns.from_midi_file(path)\n",
    "    iter = itertools.cycle(note_seq)\n",
    "\n",
    "    es1 = list\n",
    "    es2 = list\n",
    "\n",
    "    cs1 = list\n",
    "    cs2 = list\n",
    "    ev = True\n",
    "    #print(\"evaluating\")\n",
    "    for seq in iter:\n",
    "        #print(seq)\n",
    "        if ev:\n",
    "            #print(\"1st iter\")\n",
    "            seq.adjust_time(-seq.notes[0].start)\n",
    "            print(-seq.notes[0].start)\n",
    "            event_seq = es.from_note_seq(seq)\n",
    "            print(event_seq)\n",
    "            control_seq = cs.from_event_seq(event_seq)\n",
    "            es1 = event_seq.to_array()\n",
    "            cs1 = control_seq.to_compressed_array()\n",
    "            ev = False\n",
    "        else:\n",
    "            #print(\"2nd iter\")\n",
    "            seq.adjust_time(-seq.notes[0].start)\n",
    "            event_seq = es.from_note_seq(seq)\n",
    "            control_seq = cs.from_event_seq(event_seq)\n",
    "            es2 = event_seq.to_array()\n",
    "            cs2 = control_seq.to_compressed_array()\n",
    "            break\n",
    "\n",
    "    return es1, es2, cs1, cs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(utils.find_files_by_extensions('./Dataset-RAW', ['.mid', '.midi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "<Utils.EventSeq.EventSeq object at 0x132f71160>\n"
     ]
    }
   ],
   "source": [
    "es1, es2, cs1, cs2 = preprocess_midi(midi_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.client import ImproperConnectionState\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "window_size = 200\n",
    "stride_size = 10\n",
    "control_ratio = 1.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "init_dim = 32\n",
    "event_dim = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "\n",
    "def load_dataset():\n",
    "    dataset = Dataset('./ProcessedControls-RAW', verbose=True)\n",
    "    dataset_size = len(dataset.samples)\n",
    "    assert dataset_size > 0\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(root=\"./ProcessedControls-RAW\", samples=6, avglen=9079.166666666666)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 01:02:27,486 - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "2024-07-11 01:02:27,802 - DEBUG - Creating converter from 7 to 5\n",
      "2024-07-11 01:02:27,803 - DEBUG - Creating converter from 5 to 7\n",
      "2024-07-11 01:02:27,803 - DEBUG - Creating converter from 7 to 5\n",
      "2024-07-11 01:02:27,803 - DEBUG - Creating converter from 5 to 7\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "last_saving_time = time.time()\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PerformanceRNN\n",
    "\n",
    "model_artist = PerformanceRNN(240,24,32,512)\n",
    "model_artist = model_artist.to(device)\n",
    "model_AI = PerformanceRNN(240,24,32,512)\n",
    "model_AI = model_artist.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.Adam(model_artist.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model_AI.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, model_config, sess_path):\n",
    "    print('Saving to', sess_path)\n",
    "    torch.save({'model_config': model_config,\n",
    "                'model_state': model.state_dict(),\n",
    "                'model_optimizer_state': optimizer.state_dict()}, sess_path)\n",
    "    print('Done saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 5.4751667976379395, loss2:5.475212574005127\n",
      "iter 1, loss: 5.1125078201293945, loss2:5.112485408782959\n",
      "iter 2, loss: 6.118977069854736, loss2:6.119515419006348\n",
      "iter 3, loss: 5.416440963745117, loss2:5.416049957275391\n",
      "iter 4, loss: 5.041950702667236, loss2:5.041746139526367\n",
      "iter 5, loss: 4.767392158508301, loss2:4.767796516418457\n",
      "iter 6, loss: 4.612310886383057, loss2:4.612529754638672\n",
      "iter 7, loss: 4.665270805358887, loss2:4.665220260620117\n",
      "iter 8, loss: 4.540961742401123, loss2:4.540817737579346\n",
      "iter 9, loss: 4.59427547454834, loss2:4.594080924987793\n",
      "iter 10, loss: 4.486323356628418, loss2:4.486688613891602\n",
      "iter 11, loss: 4.523890495300293, loss2:4.523595809936523\n",
      "iter 12, loss: 4.38703727722168, loss2:4.3872761726379395\n",
      "iter 13, loss: 4.3582916259765625, loss2:4.357961177825928\n",
      "iter 14, loss: 4.221174716949463, loss2:4.221160888671875\n",
      "iter 15, loss: 4.191904067993164, loss2:4.191351890563965\n",
      "iter 16, loss: 4.131591320037842, loss2:4.1320414543151855\n",
      "iter 17, loss: 4.104287147521973, loss2:4.104203701019287\n",
      "iter 18, loss: 4.187987804412842, loss2:4.1875319480896\n",
      "iter 19, loss: 3.9121618270874023, loss2:3.91121768951416\n",
      "iter 20, loss: 4.004444122314453, loss2:4.004523754119873\n",
      "iter 21, loss: 4.041528224945068, loss2:4.041354179382324\n",
      "iter 22, loss: 3.913126230239868, loss2:3.913707971572876\n",
      "iter 23, loss: 3.9242937564849854, loss2:3.923490047454834\n",
      "iter 24, loss: 3.7722182273864746, loss2:3.7730987071990967\n",
      "iter 25, loss: 3.7440459728240967, loss2:3.7444236278533936\n",
      "iter 26, loss: 3.4990673065185547, loss2:3.4989490509033203\n",
      "iter 27, loss: 3.4355955123901367, loss2:3.4361329078674316\n",
      "iter 28, loss: 3.424949884414673, loss2:3.4250943660736084\n",
      "iter 29, loss: 3.2706716060638428, loss2:3.271256685256958\n",
      "Saving to ./save/Artist.sess\n",
      "Done saving\n",
      "Saving to ./save/AI.sess\n",
      "Done saving\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    batch_gen = dataset.batches(batch_size, window_size, stride_size)\n",
    "\n",
    "    for iteration, (events, controls, events2, controls2) in enumerate(batch_gen):\n",
    "\n",
    "        #1st generation\n",
    "\n",
    "        events = torch.LongTensor(events).to(device)\n",
    "        assert events.shape[0] == window_size\n",
    "\n",
    "        if np.random.random() < control_ratio:\n",
    "            controls = torch.FloatTensor(controls).to(device)\n",
    "            assert controls.shape[0] == window_size\n",
    "        else:\n",
    "            controls = None\n",
    "\n",
    "        init = torch.randn(batch_size, init_dim).to(device)\n",
    "        outputs = model_artist.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                                 teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "        assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "        #2nd generation\n",
    "\n",
    "        events2 = torch.LongTensor(events2).to(device)\n",
    "        assert events2.shape[0] == window_size\n",
    "\n",
    "        if np.random.random() < control_ratio:\n",
    "            controls2 = torch.FloatTensor(controls2).to(device)\n",
    "            assert controls2.shape[0] == window_size\n",
    "        else:\n",
    "            controls = None\n",
    "\n",
    "        init2 = torch.randn(batch_size, init_dim).to(device)\n",
    "        outputs2 = model_AI.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                                 teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "        assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "        loss = loss_function(outputs.view(-1, event_dim), events2.view(-1))\n",
    "        model_artist.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(f\"loss 1 = {loss}\")\n",
    "\n",
    "        loss2 = loss_function(outputs2.view(-1, event_dim), events2.view(-1))\n",
    "        model_AI.zero_grad()\n",
    "        loss2.backward()\n",
    "        #print(f\"loss 2 = {loss2}\")\n",
    "        norm = utils.compute_gradient_norm(model_artist.parameters())\n",
    "        nn.utils.clip_grad_norm_(model_artist.parameters(), 1.0)\n",
    "\n",
    "        norm2 = utils.compute_gradient_norm(model_AI.parameters())\n",
    "        nn.utils.clip_grad_norm_(model_AI.parameters(), 1.0)\n",
    "        \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        writer.add_scalar('model_artist/loss', loss.item(), iteration)\n",
    "        writer.add_scalar('model_artist/norm', norm.item(), iteration)\n",
    "        writer.add_scalar('model_AI/loss', loss2.item(), iteration)\n",
    "        writer.add_scalar('model_AI/norm', norm2.item(), iteration)\n",
    "\n",
    "        print(f'iter {iteration}, loss: {loss.item()}, loss2:{loss2.item()}')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    save1 = './save/Artist.sess'\n",
    "    save2 = './save/AI.sess'\n",
    "\n",
    "    save_model(model_artist, optimizer1, model_artist.state_dict(),save1)\n",
    "    save_model(model_AI, optimizer1, model_artist.state_dict(),save2)\n",
    "    print(\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_gen = 8\n",
    "mAI_path = 'save/AI.sess'\n",
    "mArtist_path = 'save/Artist.sess'\n",
    "output = 'output/'\n",
    "max_len = 2000\n",
    "greedy_ratio = 1.0\n",
    "controls = None\n",
    "control = 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'save/low_data_train.sess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mTest_model = torch.load(test, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_dim': 32,\n",
       " 'event_dim': 240,\n",
       " 'control_dim': 24,\n",
       " 'hidden_dim': 512,\n",
       " 'gru_layers': 3,\n",
       " 'gru_dropout': 0.3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mTest_model['model_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAI_state = torch.load(mAI_path, map_location=device)\n",
    "mAI = PerformanceRNN(**mTest_model['model_config']).to(device)\n",
    "mArtist_state = torch.load(mArtist_path, map_location=device)\n",
    "mArtist = PerformanceRNN(**mTest_model['model_config']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerformanceRNN(\n",
      "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
      "  (inithid_fc_activation): Tanh()\n",
      "  (event_embedding): Embedding(240, 240)\n",
      "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
      "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (gru): GRU(512, 512, num_layers=3, dropout=0.3)\n",
      "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
      "  (output_fc_activation): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mAI.load_state_dict(mAI_state['model_state'])\n",
    "mAI.eval()\n",
    "print(mAI)\n",
    "print('-' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerformanceRNN(\n",
      "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
      "  (inithid_fc_activation): Tanh()\n",
      "  (event_embedding): Embedding(240, 240)\n",
      "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
      "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (gru): GRU(512, 512, num_layers=3, dropout=0.3)\n",
      "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
      "  (output_fc_activation): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mArtist.load_state_dict(mArtist_state['model_state'])\n",
    "mArtist.eval()\n",
    "print(mArtist)\n",
    "print('-' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset2():\n",
    "    dataset = Dataset('./Line-RAW', verbose=True)\n",
    "    dataset_size = len(dataset.samples)\n",
    "    assert dataset_size > 0\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(root=\"./Line-RAW\", samples=1, avglen=14385.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_event(self, batch_size):\n",
    "        return torch.LongTensor([[self.primary_event] * batch_size]).to(device)\n",
    "\n",
    "def init_to_hidden(self, init):\n",
    "        # [batch_size, init_dim]\n",
    "        batch_size = init.shape[0]\n",
    "        out = self.inithid_fc(init)\n",
    "        out = self.inithid_fc_activation(out)\n",
    "        out = out.view(self.gru_layers, batch_size, self.hidden_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(init, steps, events=None, controls=None, greedy=1.0,\n",
    "                 temperature=1.0, teacher_forcing_ratio=1.0, output_type='index', verbose=True):\n",
    "    \n",
    "    batch_size = init.shape[0]\n",
    "    assert init.shape[1] == init_dim\n",
    "    assert steps > 0 #max_len\n",
    "\n",
    "    event = get_primary_event(batch_size)\n",
    "    hidden = init_to_hidden(init)\n",
    "\n",
    "    outputs = []\n",
    "    step_iter = range(steps)\n",
    "    if verbose:\n",
    "        step_iter = Bar('Generating').iter(step_iter)\n",
    "\n",
    "    for step in step_iter:\n",
    "        control = controls[step].unsqueeze(0) if use_control else None\n",
    "        output, hidden = self.forward(event, control, hidden)\n",
    "\n",
    "        use_greedy = np.random.random() < greedy\n",
    "        event = self._sample_event(output, greedy=use_greedy,\n",
    "                                    temperature=temperature)\n",
    "\n",
    "        if output_type == 'index':\n",
    "            outputs.append(event)\n",
    "        elif output_type == 'softmax':\n",
    "            outputs.append(self.output_fc_activation(output))\n",
    "        elif output_type == 'logit':\n",
    "            outputs.append(output)\n",
    "        else:\n",
    "            assert False\n",
    "    \n",
    "    return torch.cat(outputs, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147 142  48  41 225 143  59 197 197  51 138  55  41 197  59 197 143 142\n",
      " 152 127  55 220 220  38 197 127 197  58 197  42 197 197  31 220 124 197\n",
      " 215  29 197 197 197 220 110 219  61  60  50 197 230 197  50 220  10  50\n",
      " 220 197 197 197  61 140  55  33 210 155]\n",
      "(200, 64, 24)\n",
      "----------------------------------------------------------------------\n",
      "[ 15 220   9 220 197 197 220 197 197  59  52 197  40 230 197 210 220  35\n",
      " 140  45 197 197 210 197 197 142 220 197 197  59 138 197 220  62 140 197\n",
      " 137 197 123 197 197 146  57 197 220 147 142 138  49  47  39 140  59 220\n",
      " 126 220 197 222 131 197  52  55  46  47]\n",
      "(200, 64, 24)\n",
      "----------------------------------------------------------------------\n",
      "[210  47 150  57  40 135 197 197 197  59 152 220 197 126 197 143 122  49\n",
      " 220 220 215 197 197 210 197  62 197  54 197  54 197 220 220 220 143  55\n",
      " 197 197 220 220 197 128 115 197 197  60 130 197 140 197 220  62 217 137\n",
      " 197  47 150 197 225 144 197 197  45 222]\n",
      "(200, 64, 24)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)\n",
    "a = 0\n",
    "with torch.no_grad():\n",
    "    for iteration, (events, controls, events2, controls2) in enumerate(batch_gen):\n",
    "        print(events[-1])\n",
    "        print(controls.shape)\n",
    "        print('-'*70)\n",
    "        a += 1\n",
    "        if (a == 3):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0168,  0.1439,  0.1157,  ...,  0.0753, -0.0046, -0.1751],\n",
      "         [-0.1868, -0.1477,  0.1475,  ...,  0.1131,  0.0596,  0.1878],\n",
      "         [-0.0701,  0.1899,  0.0099,  ...,  0.0040, -0.0280, -0.3033],\n",
      "         ...,\n",
      "         [-0.1647, -0.1041,  0.2173,  ...,  0.1843, -0.1301,  0.0569],\n",
      "         [ 0.0723,  0.0560,  0.1791,  ...,  0.0599, -0.2430,  0.1499],\n",
      "         [-0.2988,  0.0029,  0.1291,  ..., -0.0918,  0.1187,  0.1205]],\n",
      "\n",
      "        [[-0.0377,  0.0994,  0.0782,  ...,  0.0400, -0.0678, -0.0984],\n",
      "         [-0.1539, -0.0200,  0.1461,  ...,  0.0798, -0.0147,  0.1074],\n",
      "         [-0.0821,  0.0928,  0.1158,  ...,  0.0106, -0.0716, -0.1778],\n",
      "         ...,\n",
      "         [-0.1606, -0.0632,  0.2021,  ...,  0.1169, -0.0907, -0.0140],\n",
      "         [ 0.0475,  0.0626,  0.1557,  ...,  0.0476, -0.2077,  0.1325],\n",
      "         [-0.2209, -0.0143,  0.1155,  ..., -0.0210, -0.0186,  0.0668]],\n",
      "\n",
      "        [[-0.0554,  0.1093,  0.0833,  ...,  0.0146, -0.0959, -0.0410],\n",
      "         [-0.1186,  0.0182,  0.1063,  ...,  0.0503, -0.0522,  0.0604],\n",
      "         [-0.0900,  0.0555,  0.1466,  ...,  0.0081, -0.0967, -0.1134],\n",
      "         ...,\n",
      "         [-0.1176, -0.0341,  0.1877,  ...,  0.0832, -0.0498, -0.0343],\n",
      "         [ 0.0136,  0.0386,  0.1360,  ...,  0.0623, -0.1409,  0.0931],\n",
      "         [-0.1270,  0.0234,  0.1248,  ..., -0.0054, -0.0833,  0.0680]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0544,  0.0197,  0.1248,  ...,  0.0092, -0.0712,  0.0108],\n",
      "         [-0.0359,  0.0322,  0.0850,  ..., -0.0186, -0.1067,  0.0715],\n",
      "         [-0.0212,  0.0415,  0.1028,  ..., -0.0150, -0.1191,  0.0514],\n",
      "         ...,\n",
      "         [-0.0460,  0.0408,  0.0777,  ...,  0.0274, -0.1146,  0.0285],\n",
      "         [-0.0156,  0.0452,  0.0925,  ...,  0.0487, -0.0768,  0.0520],\n",
      "         [-0.0187,  0.0303,  0.0843,  ..., -0.0111, -0.1040,  0.0488]],\n",
      "\n",
      "        [[-0.0583,  0.0377,  0.1091,  ..., -0.0053, -0.0847,  0.0400],\n",
      "         [-0.0376,  0.0442,  0.0790,  ..., -0.0171, -0.1089,  0.0704],\n",
      "         [-0.0128,  0.0513,  0.0804,  ..., -0.0099, -0.1348,  0.0579],\n",
      "         ...,\n",
      "         [-0.0550,  0.0341,  0.0760,  ...,  0.0108, -0.1004,  0.0201],\n",
      "         [-0.0067,  0.0378,  0.0918,  ...,  0.0493, -0.0593,  0.0503],\n",
      "         [-0.0485,  0.0106,  0.0909,  ..., -0.0031, -0.0836,  0.0392]],\n",
      "\n",
      "        [[-0.0433,  0.0209,  0.1473,  ..., -0.0020, -0.0849,  0.0393],\n",
      "         [-0.0314,  0.0438,  0.1005,  ...,  0.0074, -0.1207,  0.0815],\n",
      "         [-0.0243,  0.0533,  0.0759,  ..., -0.0087, -0.1152,  0.0413],\n",
      "         ...,\n",
      "         [-0.0419,  0.0579,  0.1003,  ...,  0.0168, -0.0895,  0.0400],\n",
      "         [-0.0381,  0.0350,  0.0888,  ...,  0.0443, -0.0613,  0.0468],\n",
      "         [-0.0213,  0.0290,  0.0918,  ..., -0.0191, -0.0941,  0.0296]]],\n",
      "       device='mps:0')\n",
      "----------------------------------------------------------------------\n",
      "torch.Size([200, 64, 240])\n",
      "----------------------------------------------------------------------\n",
      "tensor([[119, 197, 150,  ..., 197, 197, 127],\n",
      "        [197,  27, 147,  ...,  40,  54, 197],\n",
      "        [ 35, 235, 220,  ..., 220, 197,  47],\n",
      "        ...,\n",
      "        [215, 210, 225,  ...,  52,  47,  54],\n",
      "        [143, 197, 210,  ..., 197, 220, 197],\n",
      "        [215,  47, 197,  ...,  43, 135,  42]], device='mps:0')\n",
      "----------------------------------------------------------------------\n",
      "torch.Size([200, 64])\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)\n",
    "with torch.no_grad():\n",
    "    for iteration, (events, controls, events2, controls2) in enumerate(batch_gen):\n",
    "\n",
    "        #1st generation\n",
    "\n",
    "        events = torch.LongTensor(events).to(device)\n",
    "        assert events.shape[0] == window_size\n",
    "\n",
    "        if np.random.random() < control_ratio:\n",
    "            controls = torch.FloatTensor(controls).to(device)\n",
    "            assert controls.shape[0] == window_size\n",
    "        else:\n",
    "            controls = None\n",
    "\n",
    "        init = torch.randn(batch_size, init_dim).to(device)\n",
    "        outputs = model_artist.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                                    teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "        assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "        #2nd generation\n",
    "\n",
    "        events2 = torch.LongTensor(events2).to(device)\n",
    "        assert events2.shape[0] == window_size\n",
    "\n",
    "        if np.random.random() < control_ratio:\n",
    "            controls2 = torch.FloatTensor(controls2).to(device)\n",
    "            assert controls2.shape[0] == window_size\n",
    "        else:\n",
    "            controls = None\n",
    "\n",
    "        init2 = torch.randn(batch_size, init_dim).to(device)\n",
    "        outputs2 = model_AI.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                                    teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "        assert outputs2.shape[:2] == events.shape[:2]\n",
    "\n",
    "        print(outputs)\n",
    "        print('-'*70)\n",
    "        print(outputs.shape)\n",
    "        print('-'*70)\n",
    "        print(events)\n",
    "        print('-'*70)\n",
    "        print(events.shape)\n",
    "        print('-'*70)\n",
    "        outputs1 = outputs.cpu().numpy().T \n",
    "        print('-'*70)\n",
    "        outputs2 = outputs2.cpu().numpy().T \n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
